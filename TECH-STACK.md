# Technical Stack & System Architecture

**Last Updated**: 2026-02-10
**Production URL**: http://159.89.162.233:5000
**Repository**: https://github.com/akshith27hiremath/hivemind_scraper.git
**Active Branch**: `embeddings_test` (8 commits ahead of `main`)

---

## Infrastructure

### Production Environment

| Resource | Details |
|----------|---------|
| **Provider** | Digital Ocean Droplet |
| **IP** | 159.89.162.233 |
| **Specs** | 2 CPU / 4GB RAM / 58GB Disk |
| **OS** | Linux |
| **Uptime** | 45+ days (as of 2026-02-10) |
| **Disk Usage** | 22GB / 58GB (37%) |
| **Memory Usage** | 976MB / 3.8GB (25%) + 356MB swap |
| **SSH** | Key-based auth (`id_ed25519`) |

### Docker Services (All Running)

| Container | Image | Status | Ports | Purpose |
|-----------|-------|--------|-------|---------|
| `sp500_postgres` | PostgreSQL 15 Alpine | Up 6 weeks (healthy) | 5432 | Database |
| `sp500_ingestion_worker` | Python 3.11-slim | Up 6 weeks | - | News collection |
| `sp500_processing_worker` | Python 3.11-slim | Up 6 weeks | - | Classification + clustering |
| `sp500_web_dashboard` | Python 3.11-slim | Up 6 weeks | 5000 | Flask web UI + API |

### Local Development

- **Status**: All scraperMVP containers stopped (by design - production is primary)
- **Port conflict**: nulrisc project holds port 5432 locally
- **Images**: All 4 built locally (~7 weeks old)
- **Docker Compose**: `docker-compose.yml` orchestrates all services

---

## Database

### PostgreSQL 15 (Alpine)

**Connection**: `postgresql://scraper_user:***@postgres:5432/sp500_news`
**Persistent Volume**: `postgres_data`

### Tables (4 active)

#### `articles_raw` (148,268 rows, growing ~2,700/day)

The core table. Archive-first design - articles are never deleted, only annotated with metadata.

| Column | Type | Nullable | Purpose |
|--------|------|----------|---------|
| `id` | SERIAL PK | NO | Auto-increment ID |
| `url` | VARCHAR(1000) UNIQUE | NO | Deduplication key |
| `title` | TEXT | NO | Article headline |
| `summary` | TEXT | YES | Article body/excerpt |
| `source` | VARCHAR(100) | NO | Origin (e.g., "Reuters Business", "SEC EDGAR (JPM)") |
| `published_at` | TIMESTAMP | YES | Article publication date |
| `fetched_at` | TIMESTAMP | YES | When ingested (DEFAULT CURRENT_TIMESTAMP) |
| `raw_json` | JSONB | YES | Full original payload from source |
| `created_at` | TIMESTAMP | YES | DB row creation time |
| `classification_label` | VARCHAR(20) | YES | 'FACTUAL', 'OPINION', 'SLOP', or NULL |
| `classification_confidence` | DOUBLE PRECISION | YES | 0.0-1.0 model confidence |
| `classification_source` | VARCHAR(20) | YES | Always 'student' (DistilBERT) |
| `classification_model_version` | VARCHAR(50) | YES | e.g., 'bert_final' |
| `classified_at` | TIMESTAMP | YES | When classified |
| `ready_for_kg` | BOOLEAN | YES | TRUE only if FACTUAL |
| `cluster_batch_id` | UUID | YES | Links to clustering run |
| `cluster_label` | INTEGER | YES | Cluster ID; -1 = noise/unique |
| `is_cluster_centroid` | BOOLEAN | YES | TRUE = representative article |
| `distance_to_centroid` | DOUBLE PRECISION | YES | 1.0 - cosine_similarity (lower = more similar) |

**Missing column (planned)**: No `ticker` or `company_id` column. Articles are linked to companies only through the `source` field string. An NLP-based entity extraction task is pending to add proper article-company mapping.

#### `article_clusters` (43,579 rows)

Audit trail for all clustering decisions. One row per article per clustering batch.

| Column | Type | Nullable | Purpose |
|--------|------|----------|---------|
| `id` | SERIAL PK | NO | Auto-increment |
| `cluster_batch_id` | UUID | NO | Clustering run identifier |
| `article_id` | INTEGER FK | YES | References articles_raw(id) |
| `cluster_label` | INTEGER | YES | Cluster assignment (-1 = noise) |
| `is_centroid` | BOOLEAN | YES | Is this the cluster representative? |
| `distance_to_centroid` | DOUBLE PRECISION | YES | Similarity distance |
| `clustering_method` | VARCHAR | YES | Always 'embeddings' (dbscan/minhash deprecated) |
| `created_at` | TIMESTAMP | YES | When clustered |

UNIQUE constraint on `(cluster_batch_id, article_id)`.

#### `companies` (503 rows, static)

S&P 500 constituents. Seeded from `02_seed_companies.sql`.

| Column | Type | Nullable | Purpose |
|--------|------|----------|---------|
| `id` | SERIAL PK | NO | Auto-increment |
| `ticker` | VARCHAR(10) UNIQUE | NO | Stock symbol (e.g., "AAPL") |
| `name` | VARCHAR(255) | NO | Company name |
| `sector` | VARCHAR(100) | YES | e.g., "Technology" |
| `industry` | VARCHAR(100) | YES | e.g., "Consumer Electronics" |
| `cik` | VARCHAR(10) | YES | SEC CIK number |
| `created_at` | TIMESTAMP | YES | |

#### `teacher_labels` (0 rows in production)

Training labels generated by GPT-4o. 3,000 labels were generated locally but the table is empty in production (training was done on local machine).

| Column | Type | Nullable | Purpose |
|--------|------|----------|---------|
| `id` | SERIAL PK | NO | |
| `article_id` | INTEGER FK | YES | References articles_raw(id) ON DELETE CASCADE |
| `label` | VARCHAR(20) | NO | 'FACTUAL', 'OPINION', 'SLOP' |
| `confidence` | DOUBLE PRECISION | YES | Teacher model confidence |
| `reasoning` | TEXT | YES | GPT-4o explanation |
| `teacher_model` | VARCHAR(100) | YES | e.g., 'gpt-4o-2024-08-06' |
| `prompt_version` | VARCHAR(50) | YES | DEFAULT 'v1' |
| `created_at` | TIMESTAMP | YES | |

### Indexes (16 total)

| Table | Index | Column(s) | Notes |
|-------|-------|-----------|-------|
| articles_raw | `articles_raw_pkey` | id | PK |
| articles_raw | `articles_raw_url_key` | url | UNIQUE constraint |
| articles_raw | `idx_articles_url` | url | **Redundant** with url_key |
| articles_raw | `idx_articles_source` | source | Source filtering |
| articles_raw | `idx_articles_published` | published_at DESC | Time queries, clustering windows |
| articles_raw | `idx_articles_fetched` | fetched_at DESC | Recent articles |
| articles_raw | `idx_articles_classification` | classification_label | Partial: WHERE NOT NULL |
| articles_raw | `idx_articles_ready_for_kg` | ready_for_kg | Partial: WHERE TRUE |
| articles_raw | `idx_articles_cluster_batch` | cluster_batch_id | Cluster lookups |
| article_clusters | `article_clusters_pkey` | id | PK |
| article_clusters | `..._batch_id_article_id_key` | (cluster_batch_id, article_id) | UNIQUE |
| companies | `companies_pkey` | id | PK |
| companies | `companies_ticker_key` | ticker | UNIQUE constraint |
| companies | `idx_companies_ticker` | ticker | **Redundant** with ticker_key |
| companies | `idx_companies_sector` | sector | Sector filtering |
| teacher_labels | `teacher_labels_pkey` | id | PK |

**Missing indexes**: `article_clusters.article_id` (JOIN key), `article_clusters.clustering_method` (filter key).

### Current Data Volumes (Production, 2026-02-10)

| Metric | Value |
|--------|-------|
| Total articles | 148,268 |
| Classified (non-SEC) | 86,020 (58%) |
| FACTUAL | 43,579 (avg confidence 0.935) |
| OPINION | 36,011 (avg confidence 0.828) |
| SLOP | 6,430 (avg confidence 0.666) |
| Unclassified | 62,248 (mostly SEC EDGAR) |
| Cluster batches | 1,384 |
| Cluster assignments | 43,579 |
| Daily ingestion | ~2,700 articles/day |
| Peak hourly rate | 244 articles/hour (US market hours) |
| Off-peak hourly rate | 35-56 articles/hour |

### Top Sources by Volume

| Source | Articles | Date Range |
|--------|----------|------------|
| Yahoo Finance | 33,058 | 2023-12 to present |
| Investing.com | 23,187 | 2025-12 to present |
| Seeking Alpha | 14,003 | 2025-12 to present |
| Bloomberg | 5,809 | 2023-09 to present |
| SEC EDGAR (JPM) | 3,383 | 2025-12 to present |
| SEC EDGAR (MS) | 2,743 | 2025-12 to present |
| Reuters Business | 2,176 | 2025-12 to present |
| Alpha Vantage | 1,880 | 2025-10 to **2026-01-01 (STOPPED)** |

---

## Services

### 1. Ingestion Worker

**Purpose**: Collects news articles from multiple sources on a schedule.
**Entry point**: `ingestion-worker/src/main.py` -> `scheduler.py`
**Code**: ~1,814 lines across 8 files
**Connection**: SimpleConnectionPool (min=1, max=10)

#### Data Sources & Schedule

| Source | Frequency | Details |
|--------|-----------|---------|
| RSS Feeds (10) | Every 15 min | Reuters, MarketWatch, Yahoo, SA, Investing.com, CNBC, Benzinga, TechCrunch, The Verge, Bloomberg |
| Seeking Alpha Tickers | Every 4 hours | All 503 tickers |
| Finnhub API | Every 4 hours | Top 50 companies |
| Alpha Vantage | Daily at 6 AM | Top 100 companies (**currently broken since Jan 1**) |
| SEC EDGAR | Every 2 hours | All companies with CIK |

#### Deduplication

URL-based. Uses `INSERT ... ON CONFLICT (url) DO NOTHING`. Articles with the same URL are silently skipped.

#### Key Files

| File | Purpose |
|------|---------|
| `src/main.py` | Entry point (34 lines) |
| `src/scheduler.py` | Cron-like task scheduling (399 lines) |
| `src/database.py` | DatabaseManager with connection pool (318 lines) |
| `src/config.py` | RSS feed URLs, API keys, intervals (89 lines) |
| `src/parsers/rss_parser.py` | General RSS parsing |
| `src/parsers/seekingalpha_ticker_parser.py` | Ticker-specific SA feeds |
| `src/parsers/sec_parser.py` | SEC EDGAR filing parsing |
| `src/api_clients/finnhub_client.py` | Finnhub API |
| `src/api_clients/alpha_vantage_client.py` | Alpha Vantage API |

---

### 2. Processing Worker

**Purpose**: Classifies articles and clusters them for deduplication.
**Entry point**: `processing-worker/processing_scheduler.py`
**Code**: ~1,900 lines in core files + 1.8GB trained models
**Connection**: Per-operation (no pool) via ProcessingDatabaseManager

#### Pipeline Flow

```
articles_raw (unclassified, non-SEC)
    |
    v
[CLASSIFICATION] - Hourly at :00
    DistilBERT (86.5% accuracy)
    Input: "{title} {summary}"
    Output: FACTUAL / OPINION / SLOP + confidence
    Writes: classification_label, classification_confidence,
            classified_at, ready_for_kg
    |
    v (FACTUAL only, ready_for_kg=TRUE)
    |
[INCREMENTAL CLUSTERING] - Hourly at :05
    all-MiniLM-L6-v2 (384-dim embeddings)
    Greedy similarity clustering (threshold=0.5)
    36-hour publication windows
    |
    ├── Match to existing centroids
    ├── Cluster unmatched among themselves
    └── Mark singles as noise (cluster_label=-1)
    |
    Writes: cluster_batch_id, cluster_label,
            is_cluster_centroid, distance_to_centroid
    Also: INSERT into article_clusters (audit)
```

#### Classification Model

| Property | Value |
|----------|-------|
| Architecture | DistilBERT (`distilbert-base-uncased`, 66M params) |
| Accuracy | 86.50% on held-out test |
| FACTUAL F1 | 90.80% |
| OPINION F1 | 82.93% |
| SLOP F1 | 72.09% |
| Speed | ~65 articles/sec (XPU), ~30/sec (CPU) |
| Max tokens | 256 |
| Model size | 256MB (`model.safetensors`) |
| Location | `src/models/bert_classifier/final/` |
| Training data | 3,000 articles labeled by GPT-4o ($4.13 total) |

#### Clustering Algorithm

| Property | Value |
|----------|-------|
| Model | `all-MiniLM-L6-v2` (sentence-transformers) |
| Embedding dim | 384 |
| Algorithm | Greedy similarity with connectivity-based ordering |
| Threshold | 0.5 cosine similarity |
| Window | 36-hour publication windows (normalized to 12h boundaries) |
| Min cluster size | 2 |
| Input | FACTUAL articles only, SEC EDGAR excluded |
| Performance | ~1,600 articles in 3-4 seconds |
| Memory | ~1.1GB peak |
| Production results | 2,534 clusters, 21.9% dedup rate |

#### Centroid Selection

For each cluster, the article with the highest average cosine similarity to all other cluster members is selected as centroid. Single articles (noise) are marked as their own centroid with `distance_to_centroid = 0.0`.

#### Distance Convention

`distance_to_centroid = 1.0 - cosine_similarity`. Lower = more similar. 0.0 = identical.

#### Key Files

| File | Purpose |
|------|---------|
| `processing_scheduler.py` | Main hourly scheduler (248 lines) |
| `incremental_clustering.py` | Match new articles to existing centroids (366 lines) |
| `src/database.py` | ProcessingDatabaseManager (543 lines) |
| `src/mechanical_refinery/clustering.py` | SentenceEmbeddingClusterer (634 lines) |
| `src/mechanical_refinery/teacher_student/bert_classifier.py` | BertClassifier (185 lines) |
| `src/mechanical_refinery/teacher_student/filter.py` | TeacherStudentFilter (174 lines) |
| `src/mechanical_refinery/teacher_student/teacher_labeler.py` | GPT-4o API wrapper (281 lines) |
| `run_sliding_window_clustering.py` | One-time full clustering |
| `run_one_time_classification.py` | One-time full classification |
| `train_bert_classifier.py` | DistilBERT fine-tuning |
| `sandbox_labeler.py` | Flask web UI for testing classification (port 5050) |

---

### 3. Web Dashboard

**Purpose**: Flask web UI + REST API for viewing articles, clusters, and system health.
**Entry point**: `web-dashboard/app.py`
**Code**: 474 lines (app.py) + 825 lines (index.html)
**Connection**: Reuses ingestion-worker's DatabaseManager (SimpleConnectionPool 1-10)
**Framework**: Flask 3.0.0 + Bootstrap 5.3.0 + Chart.js 4.4.0

#### Current API Endpoints

| Endpoint | Purpose | Performance Notes |
|----------|---------|-------------------|
| `GET /` | Dashboard HTML | Static template |
| `GET /api/articles` | Paginated articles with filters (source, keyword, days, limit, offset) | ILIKE is slow without trigram index |
| `GET /api/sources` | All sources with counts | GROUP BY, no cache |
| `GET /api/stats` | DB statistics (total, 24h, date range, sources) | 4 separate COUNT queries |
| `GET /api/clusters` | ALL clusters across all batches | **No pagination** - loads all 43K+ rows via CTE + ARRAY_AGG |
| `GET /api/source-breakdown` | Source distribution (total + daily) | Aggregates SEC/SA variants |
| `GET /api/health` | Health check for DB, Finnhub, AlphaVantage, SEC, RSS | Calls external APIs |

#### Known Issues

- **No authentication** or rate limiting
- **No API versioning** (routes are `/api/articles` not `/api/v1/articles`)
- **Debug mode enabled** in production (`debug=True` on line 474)
- **Cluster endpoint has no pagination** - returns all clusters at once
- **Count query uses string manipulation** (fragile, line 74-77)
- **ILIKE keyword search** does full table scan (no trigram index)
- **Imports from ingestion-worker** (tight coupling)

---

## Data Flow Summary

```
[External Sources]
    RSS (10 feeds) + APIs (Finnhub, AlphaVantage, SEC) + SA Tickers
         |  every 15min - 4hr
         v
[Ingestion Worker]
    Parse → Deduplicate (URL) → INSERT articles_raw
    ~2,700 articles/day
         |
         v
[articles_raw] (148K+ rows)
         |
         v  hourly at :00
[Processing Worker - Classification]
    DistilBERT → FACTUAL/OPINION/SLOP
    ~1,900 classified/day
         |
         v  hourly at :05 (FACTUAL only)
[Processing Worker - Clustering]
    all-MiniLM-L6-v2 → cluster assignments
    Match to centroids → new clusters → noise
         |
         v
[articles_raw] (annotated with cluster metadata)
[article_clusters] (audit trail)
         |
         v
[Web Dashboard]
    Flask API → Bootstrap UI
    Articles, clusters, analytics, health
```

---

## Git & Source Control

| Property | Value |
|----------|-------|
| Remote | `https://github.com/akshith27hiremath/hivemind_scraper.git` |
| Active branch | `embeddings_test` (8 commits ahead of main, synced with origin) |
| Main branch | `main` (stale - hasn't advanced since branch point) |
| Other branches | `filtering_tests` (local-only, 3 docs commits, likely abandoned) |
| Working tree | Clean (1 modified: `.claude/settings.local.json`) |

### Untracked Items

| Item | Size | Notes |
|------|------|-------|
| `database_dumps/` | 4.6MB | Production backup (2025-12-20) |
| `processing-worker/src/models/` | 1.8GB | DistilBERT weights + checkpoints |
| `docs/` | Small | DATABASE_ACCESS.md |
| `docker-compose.yml.backup` | Small | Backup config |
| `nul` | 0 bytes | Windows artifact, should delete |

### Recommended Git Housekeeping

1. Merge `embeddings_test` -> `main` (main doesn't reflect production)
2. Delete or push `filtering_tests`
3. Add to `.gitignore`: `database_dumps/`, `nul`, `*.backup`
4. Consider `.gitignore` for `processing-worker/src/models/` (1.8GB)

---

## Dependencies

### Ingestion Worker (7 packages)

```
psycopg2-binary, feedparser, requests, beautifulsoup4,
schedule, python-dotenv, lxml
```

### Processing Worker (26 packages, ML-heavy)

```
psycopg2-binary, python-dotenv, torch, transformers,
sentence-transformers, scikit-learn, numpy,
openai (for teacher labeling), flask (for sandbox)
```

### Web Dashboard (5 packages)

```
Flask==3.0.0, psycopg2-binary==2.9.9, python-dotenv==1.0.0,
requests==2.31.0, feedparser==6.0.11 (unused in app.py)
```

---

## Environment Variables (.env)

```
POSTGRES_HOST=postgres          # 'localhost' for local dev
POSTGRES_PORT=5432
POSTGRES_DB=sp500_news
POSTGRES_USER=scraper_user
POSTGRES_PASSWORD=***
FINNHUB_API_KEY=***
ALPHAVANTAGE_API_KEY=***
OPENAI_API_KEY=***              # For teacher labeling only
FETCH_INTERVAL_MINUTES=15
LOG_LEVEL=INFO
```

---

## Known Issues & Technical Debt

| Issue | Severity | Details |
|-------|----------|---------|
| `main` branch stale | Medium | 8 commits behind `embeddings_test`, doesn't reflect production |
| Alpha Vantage stopped | Low | Last ingestion 2026-01-01, API key may have expired |
| Debug mode in production | Medium | `app.run(debug=True)` in web-dashboard |
| No article-company FK | High (for API) | Articles linked to companies only via `source` string parsing |
| Cluster endpoint no pagination | Medium | `/api/clusters` loads all 43K+ rows |
| No API auth | High (for API) | No authentication on any endpoint |
| Redundant indexes | Low | `idx_articles_url` and `idx_companies_ticker` duplicate UNIQUE constraints |
| Processing worker no connection pool | Low | Creates new connection per operation |
| teacher_labels empty in prod | Low | Training data was generated locally, not synced to production |
| `nul` file in repo root | Low | Windows artifact, should delete |
| CLAUDE.md stats outdated | Low | Documents 52K articles, production has 148K |

---

## Architecture Principles

1. **Archive-First**: All articles preserved in `articles_raw`, never deleted. Processing adds metadata columns.
2. **Classification Before Clustering**: Pipeline order is always classify -> filter FACTUAL -> cluster.
3. **SEC EDGAR Excluded**: Form 4/8-K filings are excluded from both classification and clustering.
4. **36-Hour Windows**: Clustering groups articles published within the same 36-hour window.
5. **UUID as String**: Always convert `uuid.uuid4()` to `str()` for psycopg2 compatibility.
6. **Incremental Processing**: Hourly scheduler processes only recent articles (2-hour lookback).
7. **Centroid = Representative**: Each cluster's centroid is the article most similar to all others in the cluster.
